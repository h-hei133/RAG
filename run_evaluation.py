"""
RAG ç³»ç»Ÿè´¨é‡è¯„ä¼°è„šæœ¬
è¿è¡Œæ–¹å¼: python run_evaluation.py
"""

import json
import os
from src.rag_chain import get_rag_chain
from src.evaluation import RAGEvaluator, EvaluationSample, EvaluationResult

# ========== é…ç½® ==========
# æµ‹è¯•é—®é¢˜åˆ—è¡¨ (è¯·æ ¹æ®ä½ çš„çŸ¥è¯†åº“å†…å®¹ä¿®æ”¹)
# âš ï¸ é‡è¦ï¼šé—®é¢˜å¿…é¡»ä¸Žå®žé™…ä¸Šä¼ çš„æ–‡æ¡£å†…å®¹ç›¸å…³ï¼
# é€šç”¨é—®é¢˜å¦‚"æ€»ç»“æ–‡æ¡£"ä¼šå¯¼è‡´æ£€ç´¢å¤±è´¥

# ç¤ºä¾‹ï¼šé’ˆå¯¹ Terrapin Attack è®ºæ–‡çš„é—®é¢˜
TEST_QUESTIONS_TERRAPIN = [
    "ä»€ä¹ˆæ˜¯ Terrapin æ”»å‡»ï¼Ÿå®ƒåˆ©ç”¨äº†ä»€ä¹ˆæ¼æ´žï¼Ÿ",
    "Terrapin æ”»å‡»å¦‚ä½•å½±å“ SSH åè®®çš„å®‰å…¨æ€§ï¼Ÿ",
    "å¦‚ä½•é˜²å¾¡ Terrapin æ”»å‡»ï¼Ÿæœ‰å“ªäº›è¡¥ä¸æŽªæ–½ï¼Ÿ",
    "AsyncSSH åœ¨ Terrapin æ”»å‡»ä¸­çš„æ¼æ´žæ˜¯ä»€ä¹ˆï¼Ÿ",
]

# ç¤ºä¾‹ï¼šé’ˆå¯¹ã€Šæ•°è®ºåŸºç¡€ã€‹çš„é—®é¢˜
TEST_QUESTIONS_NUMBER_THEORY = [
    "æ•°è®ºåŸºç¡€è¿™æœ¬ä¹¦åŒ…å«å“ªäº›ç« èŠ‚å†…å®¹ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é™¤æ•°å‡½æ•° d(n)ï¼Ÿå®ƒæœ‰ä»€ä¹ˆæ€§è´¨ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯åŽŸæ ¹å’ŒæŒ‡æ ‡ï¼Ÿ",
]

# å½“å‰ä½¿ç”¨çš„æµ‹è¯•é—®é¢˜ (æ ¹æ®ä½ ä¸Šä¼ çš„æ–‡æ¡£é€‰æ‹©)
TEST_QUESTIONS = TEST_QUESTIONS_TERRAPIN + TEST_QUESTIONS_NUMBER_THEORY

# å¦‚æžœæœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå¯ä»¥æ·»åŠ  ground_truth
# TEST_DATA = [
#     {"question": "é—®é¢˜1", "ground_truth": "æ ‡å‡†ç­”æ¡ˆ1"},
#     {"question": "é—®é¢˜2", "ground_truth": "æ ‡å‡†ç­”æ¡ˆ2"},
# ]


def run_single_evaluation(chain, evaluator, question: str, ground_truth: str = None):
    """è¯„ä¼°å•ä¸ªé—®é¢˜"""
    print(f"\nðŸ” é—®é¢˜: {question}")
    
    # è°ƒç”¨ RAG
    result = chain.invoke({"input": question, "chat_history": []})
    answer = result["answer"]
    contexts = [doc.page_content for doc in result.get("source_documents", [])]
    
    print(f"ðŸ’¬ å›žç­”: {answer[:200]}..." if len(answer) > 200 else f"ðŸ’¬ å›žç­”: {answer}")
    
    # è¯„ä¼°
    sample = EvaluationSample(
        question=question,
        answer=answer,
        contexts=contexts,
        ground_truth=ground_truth
    )
    
    eval_result = evaluator.evaluate_sample(sample)
    
    print(f"ðŸ“Š è¯„åˆ†:")
    print(f"   Faithfulness: {eval_result.faithfulness:.2f}" if eval_result.faithfulness else "   Faithfulness: N/A")
    print(f"   Relevancy: {eval_result.answer_relevancy:.2f}" if eval_result.answer_relevancy else "   Relevancy: N/A")
    print(f"   Precision: {eval_result.context_precision:.2f}" if eval_result.context_precision else "   Precision: N/A")
    print(f"   ç»¼åˆ: {eval_result.overall_score:.2f}" if eval_result.overall_score else "   ç»¼åˆ: N/A")
    
    return eval_result


def run_batch_evaluation():
    """æ‰¹é‡è¯„ä¼°"""
    print("=" * 60)
    print("ðŸš€ RAG ç³»ç»Ÿè´¨é‡è¯„ä¼°")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    chain = get_rag_chain()
    if not chain:
        print("âŒ RAG é“¾åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼")
        return
    
    evaluator = RAGEvaluator(llm=chain.llm)
    
    # è¿è¡Œè¯„ä¼°
    results = []
    for question in TEST_QUESTIONS:
        try:
            result = run_single_evaluation(chain, evaluator, question)
            results.append(result)
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ðŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
    print("=" * 60)
    
    metrics = evaluator.get_aggregate_metrics()
    for key, value in metrics.items():
        if value is not None:
            print(f"  {key}: {value:.2f}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = "./logs/evaluation_report.json"
    os.makedirs("./logs", exist_ok=True)
    
    report = {
        "total_samples": len(results),
        "aggregate_metrics": metrics,
        "details": [r.to_dict() for r in results]
    }
    
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def interactive_evaluation():
    """äº¤äº’å¼è¯„ä¼° - é€ä¸ªé—®é¢˜æµ‹è¯•"""
    print("=" * 60)
    print("ðŸ”¬ äº¤äº’å¼ RAG è¯„ä¼°")
    print("è¾“å…¥é—®é¢˜è¿›è¡Œè¯„ä¼°ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60)
    
    chain = get_rag_chain()
    if not chain:
        print("âŒ RAG é“¾åˆå§‹åŒ–å¤±è´¥ï¼")
        return
    
    evaluator = RAGEvaluator(llm=chain.llm)
    
    while True:
        question = input("\nðŸ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
        if question.lower() in ['quit', 'exit', 'q']:
            break
        if not question:
            continue
        
        try:
            run_single_evaluation(chain, evaluator, question)
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    print("\nðŸ‘‹ è¯„ä¼°ç»“æŸ")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        interactive_evaluation()
    else:
        run_batch_evaluation()
