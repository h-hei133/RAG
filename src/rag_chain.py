import os
import torch
import pickle
import sqlite3
import config
import streamlit as st
from operator import itemgetter

# LangChain ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage

# æ£€ç´¢ç›¸å…³ç»„ä»¶
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
import json
import time
import uuid
from typing import Literal, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

def calculate_mrr(retrieved_ids: List[str], relevant_id: str) -> float:
    """
    è®¡ç®—å•ä¸ªæŸ¥è¯¢çš„å€’æ•°æ’å (Reciprocal Rank)
    è¿”å› 1/Kï¼Œå…¶ä¸­ K æ˜¯ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’åä½ç½®
    å¦‚æœç›¸å…³æ–‡æ¡£ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè¿”å› 0
    """
    try:
        rank = retrieved_ids.index(relevant_id) + 1
        return 1.0 / rank
    except ValueError:
        return 0.0


def log_feedback(run_id: str, score: int, comment: Optional[str] = None):
    """
    è®°å½•ç”¨æˆ·åé¦ˆåˆ°æ—¥å¿—æ–‡ä»¶
    score: 1 = æ­£é¢åé¦ˆ (ğŸ‘), 0 = è´Ÿé¢åé¦ˆ (ğŸ‘)
    """
    import config
    feedback_log = {
        "run_id": run_id,
        "score": score,
        "comment": comment,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    os.makedirs(os.path.dirname(config.FEEDBACK_LOG_PATH), exist_ok=True)
    with open(config.FEEDBACK_LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(feedback_log, ensure_ascii=False) + "\n")

from langchain_core.pydantic_v1 import BaseModel, Field

# ========== CRAG çº é”™æ£€ç´¢ (Corrective RAG) ==========
class GradeDocuments(BaseModel):
    """æ£€ç´¢ç»“æœç›¸å…³æ€§è¯„åˆ†"""
    binary_score: str = Field(
        description="æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³, 'yes' æˆ– 'no'"
    )
    reasoning: str = Field(
        description="è¯„åˆ†ç†ç”±",
        default=""
    )


class DocumentGrader:
    """
    CRAG æ–‡æ¡£è¯„åˆ†å™¨ï¼šè¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œè¿‡æ»¤ä¸ç›¸å…³æ–‡æ¡£
    å½“æ£€ç´¢è´¨é‡å·®æ—¶è§¦å‘å›é€€ç­–ç•¥
    """
    
    GRADING_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚åˆ¤æ–­ä¸‹é¢çš„æ–‡æ¡£æ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³ã€‚

ç”¨æˆ·é—®é¢˜: {question}

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{document}

è¯„åˆ¤æ ‡å‡†:
1. æ–‡æ¡£æ˜¯å¦åŒ…å«ä¸é—®é¢˜ç›¸å…³çš„å…³é”®è¯æˆ–è¯­ä¹‰ä¿¡æ¯
2. æ–‡æ¡£å†…å®¹æ˜¯å¦èƒ½å¸®åŠ©å›ç­”è¿™ä¸ªé—®é¢˜
3. å³ä½¿åªæ˜¯éƒ¨åˆ†ç›¸å…³ä¹Ÿåº”è¯¥åˆ¤å®šä¸ºç›¸å…³

è¯·è¿”å› JSON æ ¼å¼:
{{"binary_score": "yes/no", "reasoning": "ç®€çŸ­ç†ç”±"}}"""
    
    def __init__(self, llm):
        self.llm = llm
        self.output_parser = StrOutputParser()
    
    def grade_document(self, question: str, doc_content: str) -> bool:
        """
        è¯„ä¼°å•ä¸ªæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
        è¿”å›: True=ç›¸å…³, False=ä¸ç›¸å…³
        """
        try:
            prompt = self.GRADING_PROMPT.format(
                question=question,
                document=doc_content[:1000]  # é™åˆ¶é•¿åº¦
            )
            response = self.llm.invoke(prompt)
            result_text = self.output_parser.invoke(response)
            
            # è§£æ JSON
            import re
            json_match = re.search(r'\{[^}]+\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result.get("binary_score", "no").lower() == "yes"
            return True  # è§£æå¤±è´¥æ—¶ä¿å®ˆå¤„ç†
        except Exception as e:
            print(f"æ–‡æ¡£è¯„åˆ†å¤±è´¥: {e}")
            return True  # å¤±è´¥æ—¶ä¿å®ˆå¤„ç†
    
    def grade_and_filter(self, question: str, docs: list, threshold: float = 0.5) -> tuple:
        """
        CRAG æ ¸å¿ƒé€»è¾‘ï¼šè¯„ä¼°å¹¶è¿‡æ»¤æ£€ç´¢ç»“æœ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            threshold: ä¸ç›¸å…³æ–‡æ¡£æ¯”ä¾‹é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™è§¦å‘å›é€€
            
        Returns:
            (filtered_docs, need_fallback, stats)
        """
        if not docs:
            return [], True, {"total": 0, "relevant": 0, "irrelevant": 0}
        
        filtered_docs = []
        irrelevant_count = 0
        
        for doc in docs:
            is_relevant = self.grade_document(question, doc.page_content)
            if is_relevant:
                filtered_docs.append(doc)
            else:
                irrelevant_count += 1
        
        total = len(docs)
        relevant_count = total - irrelevant_count
        need_fallback = (irrelevant_count / total) > threshold if total > 0 else True
        
        stats = {
            "total": total,
            "relevant": relevant_count,
            "irrelevant": irrelevant_count,
            "relevance_ratio": relevant_count / total if total > 0 else 0
        }
        
        return filtered_docs, need_fallback, stats


# ========== è¯­ä¹‰ç¼“å­˜ (Semantic Cache) ==========
class SemanticCache:
    """
    è¯­ä¹‰ç¼“å­˜ï¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åŒ¹é…ç¼“å­˜
    ç›¸æ¯”å­—ç¬¦ä¸²åŒ¹é…ï¼Œå¯ä»¥è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜
    ä¾‹å¦‚: "How are you?" å’Œ "How are you" ä¼šå‘½ä¸­åŒä¸€ç¼“å­˜
    """
    
    def __init__(self, embeddings, threshold: float = 0.92, max_size: int = 1000):
        """
        Args:
            embeddings: åµŒå…¥æ¨¡å‹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)ï¼Œè¶Šé«˜è¶Šä¸¥æ ¼
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        """
        self.embeddings = embeddings
        self.threshold = threshold
        self.max_size = max_size
        self.cache_path = "./logs/semantic_cache.pkl"
        self.cache_vectors = []  # [(question, question_vector, answer, timestamp)]
        self._load_cache()
    
    def _load_cache(self):
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜"""
        import pickle
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, "rb") as f:
                    self.cache_vectors = pickle.load(f)
                print(f"âœ… è¯­ä¹‰ç¼“å­˜å·²åŠ è½½: {len(self.cache_vectors)} æ¡")
            except Exception as e:
                print(f"åŠ è½½è¯­ä¹‰ç¼“å­˜å¤±è´¥: {e}")
                self.cache_vectors = []
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜"""
        import pickle
        os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)
        try:
            with open(self.cache_path, "wb") as f:
                pickle.dump(self.cache_vectors, f)
        except Exception as e:
            print(f"ä¿å­˜è¯­ä¹‰ç¼“å­˜å¤±è´¥: {e}")
    
    def _cosine_similarity(self, vec1, vec2) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)
    
    def get(self, question: str) -> Optional[str]:
        """
        è¯­ä¹‰ç¼“å­˜æŸ¥è¯¢
        è¿”å›: ç¼“å­˜çš„ç­”æ¡ˆï¼Œæœªå‘½ä¸­è¿”å› None
        """
        if not self.cache_vectors:
            return None
        
        try:
            q_vec = self.embeddings.embed_query(question)
            
            best_match = None
            best_similarity = 0
            
            for cached_q, cached_vec, answer, _ in self.cache_vectors:
                similarity = self._cosine_similarity(q_vec, cached_vec)
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = (cached_q, answer)
            
            if best_similarity >= self.threshold:
                print(f"ğŸ¯ è¯­ä¹‰ç¼“å­˜å‘½ä¸­: {best_similarity:.2%} ç›¸ä¼¼åº¦")
                return best_match[1]
            
            return None
        except Exception as e:
            print(f"è¯­ä¹‰ç¼“å­˜æŸ¥è¯¢å¤±è´¥: {e}")
            return None
    
    def set(self, question: str, answer: str):
        """æ·»åŠ åˆ°è¯­ä¹‰ç¼“å­˜"""
        try:
            q_vec = self.embeddings.embed_query(question)
            timestamp = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨éå¸¸ç›¸ä¼¼çš„é—®é¢˜
            for i, (cached_q, cached_vec, _, _) in enumerate(self.cache_vectors):
                similarity = self._cosine_similarity(q_vec, cached_vec)
                if similarity >= 0.98:  # å‡ ä¹å®Œå…¨ç›¸åŒï¼Œæ›´æ–°ç­”æ¡ˆ
                    self.cache_vectors[i] = (question, q_vec, answer, timestamp)
                    self._save_cache()
                    return
            
            # æ·»åŠ æ–°ç¼“å­˜
            self.cache_vectors.append((question, q_vec, answer, timestamp))
            
            # é™åˆ¶ç¼“å­˜å¤§å° (LRU: åˆ é™¤æœ€æ—§çš„)
            if len(self.cache_vectors) > self.max_size:
                self.cache_vectors.sort(key=lambda x: x[3])  # æŒ‰æ—¶é—´æ’åº
                self.cache_vectors = self.cache_vectors[-self.max_size:]
            
            self._save_cache()
        except Exception as e:
            print(f"è¯­ä¹‰ç¼“å­˜å†™å…¥å¤±è´¥: {e}")


# ========== Token ç®¡ç†å™¨ ==========
class TokenManager:
    """
    Token ç®¡ç†å™¨ï¼šç¡®ä¿ä¸Šä¸‹æ–‡ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
    æ”¯æŒ tiktoken ç²¾ç¡®è®¡æ•°å’Œå­—ç¬¦ä¼°ç®—å›é€€
    """
    
    def __init__(self, model_name: str = "gpt-4", max_context_tokens: int = 6000):
        """
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œç”¨äºé€‰æ‹©æ­£ç¡®çš„ tokenizer
            max_context_tokens: ä¸Šä¸‹æ–‡æœ€å¤§ token æ•°
        """
        self.model_name = model_name
        self.max_context_tokens = max_context_tokens
        self.encoder = None
        self._init_encoder()
    
    def _init_encoder(self):
        """åˆå§‹åŒ– tokenizer"""
        try:
            import tiktoken
            # å°è¯•è·å–æ¨¡å‹å¯¹åº”çš„ç¼–ç å™¨
            try:
                self.encoder = tiktoken.encoding_for_model(self.model_name)
            except KeyError:
                # å¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼Œä½¿ç”¨ cl100k_base (GPT-4 ç³»åˆ—)
                self.encoder = tiktoken.get_encoding("cl100k_base")
        except ImportError:
            print("âš ï¸ tiktoken æœªå®‰è£…ï¼Œä½¿ç”¨å­—ç¬¦ä¼°ç®— token æ•°")
            self.encoder = None
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°"""
        if self.encoder:
            return len(self.encoder.encode(text))
        else:
            # ä¼°ç®—ï¼šä¸­æ–‡çº¦ 2 å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦/token
            # ä¿å®ˆä¼°è®¡ä½¿ç”¨ 2
            return len(text) // 2
    
    def trim_context(self, context_str: str, reserve_tokens: int = 500) -> str:
        """
        è£å‰ªä¸Šä¸‹æ–‡ä»¥é€‚åº” token é™åˆ¶
        
        Args:
            context_str: åŸå§‹ä¸Šä¸‹æ–‡
            reserve_tokens: ä¸ºé—®é¢˜å’Œå›ç­”ä¿ç•™çš„ token æ•°
            
        Returns:
            è£å‰ªåçš„ä¸Šä¸‹æ–‡
        """
        max_allowed = self.max_context_tokens - reserve_tokens
        current_tokens = self.count_tokens(context_str)
        
        if current_tokens <= max_allowed:
            return context_str
        
        # éœ€è¦è£å‰ª
        print(f"âš ï¸ ä¸Šä¸‹æ–‡è¿‡é•¿ ({current_tokens} tokens)ï¼Œæ­£åœ¨è£å‰ªè‡³ {max_allowed} tokens")
        
        if self.encoder:
            # ç²¾ç¡®è£å‰ª
            tokens = self.encoder.encode(context_str)
            truncated_tokens = tokens[:max_allowed]
            truncated_text = self.encoder.decode(truncated_tokens)
        else:
            # å­—ç¬¦ä¼°ç®—è£å‰ª
            char_limit = max_allowed * 2
            truncated_text = context_str[:char_limit]
        
        return truncated_text + "\n\n[æ³¨: ä¸Šä¸‹æ–‡å·²æˆªæ–­ä»¥é€‚åº”æ¨¡å‹é™åˆ¶]"
    
    def trim_documents(self, docs: list, max_docs: int = 5) -> list:
        """
        é™åˆ¶æ–‡æ¡£æ•°é‡å’Œæ€»é•¿åº¦
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_docs: æœ€å¤§æ–‡æ¡£æ•°
            
        Returns:
            è£å‰ªåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if len(docs) <= max_docs:
            return docs
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆå‡è®¾å·²ç»æ’åºï¼‰ï¼Œå–å‰ N ä¸ª
        return docs[:max_docs]

class RouteQuery(BaseModel):
    """ç”¨æˆ·æŸ¥è¯¢æ„å›¾åˆ†ç±»"""
    intent: Literal["GREETING", "SIMPLE", "COMPLEX", "ABSTRACT", "METADATA_QUERY", "COMPARE", "SUMMARIZE", "OUT_OF_DOMAIN"] = Field(
        description="æŸ¥è¯¢æ„å›¾ç±»å‹"
    )
    reasoning: str = Field(
        description="åˆ†ç±»ç†ç”±",
        default=""
    )

class QueryPlanner:
    """
    Query è§„åˆ’å™¨ï¼šè´Ÿè´£ Query é‡å†™ã€åˆ†å‘ã€HyDE ç­‰é«˜çº§æ£€ç´¢ç­–ç•¥
    """
    def __init__(self, llm):
        self.llm = llm
        self.output_parser = StrOutputParser()

    def classify_intent(self, question: str) -> str:
        """
        ä½¿ç”¨ LLM å¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œæ„å›¾åˆ†ç±»
        è¿”å›: GREETING | SIMPLE | COMPLEX | ABSTRACT | METADATA_QUERY | COMPARE | SUMMARIZE | OUT_OF_DOMAIN
        """
        routing_prompt = f"""åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­å…¶æ„å›¾ç±»å‹ã€‚

é—®é¢˜: "{question}"

ç±»å‹åˆ¤æ–­æ ‡å‡†:
1. GREETING: æ‰“æ‹›å‘¼æˆ–é—²èŠï¼ˆå¦‚ï¼šä½ å¥½ã€è°¢è°¢ã€å†è§ã€hiã€helloï¼‰
2. SIMPLE: äº‹å®æ€§ç®€å•é—®é¢˜ï¼Œåªéœ€å•ä¸€æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚ï¼šXXæ˜¯ä»€ä¹ˆæ—¶å€™å‘å¸ƒçš„ï¼Ÿï¼‰
3. COMPLEX: æ¶‰åŠå¤šè·³æ¨ç†æˆ–éœ€è¦å¤šè§’åº¦å›ç­”ï¼ˆå¦‚ï¼šè¿™ä¸ªæŠ€æœ¯å¦‚ä½•å½±å“XXï¼Ÿï¼‰
4. ABSTRACT: æ¦‚å¿µæ€§é—®é¢˜ï¼Œé€‚åˆå…ˆç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆå†æ£€ç´¢ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿå¦‚ä½•ç†è§£XXï¼Ÿï¼‰
5. METADATA_QUERY: è¯¢é—®çŸ¥è¯†åº“å…ƒä¿¡æ¯ï¼ˆå¦‚ï¼šæœ‰å“ªäº›æ–‡æ¡£ï¼Ÿæ–‡ä»¶åˆ—è¡¨ï¼Ÿï¼‰
6. COMPARE: å¯¹æ¯”ç±»é—®é¢˜ï¼ˆå¦‚ï¼šAå’ŒBæœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸXå¥½è¿˜æ˜¯Yå¥½ï¼Ÿï¼‰
7. SUMMARIZE: æ€»ç»“ç±»é—®é¢˜ï¼ˆå¦‚ï¼šæ€»ç»“è¿™ç¯‡æ–‡æ¡£ã€æ¦‚æ‹¬ä¸»è¦å†…å®¹ï¼‰
8. OUT_OF_DOMAIN: ä¸çŸ¥è¯†åº“æ— å…³çš„é—®é¢˜ï¼ˆå¦‚ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿï¼‰

è¯·åªè¿”å›ä¸€ä¸ª JSON æ ¼å¼:
{{"intent": "ç±»å‹", "reasoning": "ç†ç”±"}}"""
        try:
            response = self.llm.invoke(routing_prompt)
            result_text = self.output_parser.invoke(response)
            # å°è¯•è§£æ JSON
            import json
            import re
            # æå– JSON éƒ¨åˆ†
            json_match = re.search(r'\{[^}]+\}', result_text)
            if json_match:
                result = json.loads(json_match.group())
                intent = result.get("intent", "SIMPLE").upper()
                valid_intents = ["GREETING", "SIMPLE", "COMPLEX", "ABSTRACT", 
                                "METADATA_QUERY", "COMPARE", "SUMMARIZE", "OUT_OF_DOMAIN"]
                if intent in valid_intents:
                    return intent
            return "SIMPLE"
        except Exception as e:
            print(f"æ„å›¾åˆ†ç±»å¤±è´¥: {e}")
            return "SIMPLE"
        except Exception as e:
            print(f"æ„å›¾åˆ†ç±»å¤±è´¥: {e}")
            return "SIMPLE"

    def plan(self, question: str, chat_history: list) -> dict:
        """
        å¯¹ Query è¿›è¡Œæ„å›¾åˆ†æå’Œè§„åˆ’
        """
        intent = self.classify_intent(question)
        
        result = {
            "type": intent,
            "queries": [question],
            "use_hyde": intent == "ABSTRACT",
            "sub_questions": [],
            "skip_retrieval": False
        }
        
        # æ ¹æ®æ„å›¾ç±»å‹é€‰æ‹©ä¸åŒç­–ç•¥
        if intent == "GREETING":
            result["skip_retrieval"] = True
        elif intent == "OUT_OF_DOMAIN":
            result["skip_retrieval"] = True
        elif intent == "METADATA_QUERY":
            result["skip_retrieval"] = True  # ç›´æ¥æŸ¥è¯¢æ•°æ®åº“å…ƒä¿¡æ¯
        elif intent in ["COMPLEX", "COMPARE"]:
            # å¤æ‚/å¯¹æ¯”é—®é¢˜ä½¿ç”¨å­é—®é¢˜åˆ†è§£
            sub_questions = self.decompose_complex_query(question)
            result["sub_questions"] = sub_questions
            result["queries"] = [question] + sub_questions
        elif intent == "SIMPLE":
            # SIMPLE é—®é¢˜ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
            variants = self.expand_query(question)
            result["queries"] = [question] + variants
        elif intent == "SUMMARIZE":
            # æ€»ç»“é—®é¢˜ï¼šä¸æ‰©å±•æŸ¥è¯¢ï¼Œä½¿ç”¨æ›´å¤§çš„æ£€ç´¢èŒƒå›´
            result["queries"] = [question]
        # ABSTRACT: ä½¿ç”¨ HyDEï¼Œå·²åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
        
        return result

    def expand_query(self, question: str) -> List[str]:
        """
        ç”ŸæˆæŸ¥è¯¢å˜ä½“ï¼Œç”¨äºå¤šè·¯å¬å›
        è¿”å› 3 ä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„æœç´¢è¯
        """
        prompt = f"""é’ˆå¯¹ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œè¯·ç”Ÿæˆ3ä¸ªä¸åŒè§’åº¦çš„æœç´¢æŸ¥è¯¢è¯ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¦æ±‚:
1. æ¯ä¸ªæŸ¥è¯¢è¯å ä¸€è¡Œ
2. æ¶µç›–é—®é¢˜çš„ä¸åŒæ–¹é¢
3. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ç»„åˆ
4. åªè¿”å›æŸ¥è¯¢è¯ï¼Œä¸è¦ç¼–å·æˆ–è§£é‡Š

ç¤ºä¾‹:
é—®é¢˜: "åä¸ºå’Œå°ç±³çš„æ‰‹æœºå“ªä¸ªå¥½ï¼Ÿ"
åä¸ºæ‰‹æœºå‚æ•°é…ç½®
å°ç±³æ‰‹æœºæ€§èƒ½è¯„æµ‹
åä¸ºå°ç±³å¯¹æ¯”åˆ†æ"""
        
        try:
            response = self.llm.invoke(prompt)
            result = self.output_parser.invoke(response)
            variants = [v.strip() for v in result.strip().split("\n") if v.strip()]
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„å˜ä½“
            variants = [v for v in variants if 2 < len(v) < 50]
            return variants[:3]  # æœ€å¤šè¿”å›3ä¸ª
        except Exception as e:
            print(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return []

    def generate_hyde_doc(self, question: str) -> str:
        """
        ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ (HyDE - Hypothetical Document Embeddings)
        ç”¨äºæŠ½è±¡æ¦‚å¿µé—®é¢˜çš„æ£€ç´¢å¢å¼º
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ä½œè€…ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œå†™ä¸€æ®µä¸“ä¸šã€å‡†ç¡®çš„å›ç­”è‰ç¨¿ã€‚
è¿™æ®µå›ç­”å°†ç”¨äºå¸®åŠ©æœç´¢å¼•æ“æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ‰€ä»¥è¯·åŒ…å«å°½å¯èƒ½å¤šçš„ä¸“ä¸šæœ¯è¯­å’Œå…³é”®æ¦‚å¿µã€‚

é—®é¢˜: {question}

è¦æ±‚:
1. 100-200å­—å·¦å³
2. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
3. æ¶µç›–æ ¸å¿ƒæ¦‚å¿µ
4. ä¸è¦è¯´"æˆ‘ä¸çŸ¥é“"ä¹‹ç±»çš„è¯"""
        
        try:
            response = self.llm.invoke(prompt)
            return self.output_parser.invoke(response)
        except Exception as e:
            print(f"HyDEç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def decompose_complex_query(self, question: str) -> List[str]:
        """
        å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯ç‹¬ç«‹å›ç­”çš„å­é—®é¢˜
        é€‚ç”¨äºå¯¹æ¯”ç±»ã€å¤šè·³æ¨ç†ç±»é—®é¢˜
        ä¾‹å¦‚: "åä¸ºå’Œå°ç±³çš„æ‰‹æœºå“ªä¸ªå¥½?" -> ["åä¸ºæ‰‹æœºæœ‰ä»€ä¹ˆç‰¹ç‚¹?", "å°ç±³æ‰‹æœºæœ‰ä»€ä¹ˆç‰¹ç‚¹?", "åä¸ºå°ç±³æ‰‹æœºå¯¹æ¯”"]
        """
        prompt = f"""å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸º2-4ä¸ªå¯ç‹¬ç«‹å›ç­”çš„å­é—®é¢˜ã€‚
æ¯ä¸ªå­é—®é¢˜åº”è¯¥å¯ä»¥é€šè¿‡å•ç‹¬çš„çŸ¥è¯†åº“æ£€ç´¢æ¥å›ç­”ã€‚

é—®é¢˜: {question}

è¦æ±‚:
1. æ¯ä¸ªå­é—®é¢˜å ä¸€è¡Œ
2. å­é—®é¢˜åº”è¯¥ç®€æ´æ˜äº†
3. ä¸è¦ç¼–å·
4. ä¸è¦æ·»åŠ è§£é‡Š

ç¤ºä¾‹:
é—®é¢˜: "æ¯”è¾ƒPythonå’ŒJavaåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„åº”ç”¨"
Pythonåœ¨æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŠ¿
Javaåœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨
Pythonå’ŒJavaæ€§èƒ½å¯¹æ¯”"""
        
        try:
            response = self.llm.invoke(prompt)
            result = self.output_parser.invoke(response)
            sub_questions = [q.strip() for q in result.strip().split("\n") if q.strip()]
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„å­é—®é¢˜
            sub_questions = [q for q in sub_questions if 4 < len(q) < 100]
            return sub_questions[:4]  # æœ€å¤šè¿”å›4ä¸ª
        except Exception as e:
            print(f"å­é—®é¢˜åˆ†è§£å¤±è´¥: {e}")
            return [question]  # å¤±è´¥æ—¶è¿”å›åŸé—®é¢˜


class ManualHistoryRAGChain:
    """
    æ‰‹åŠ¨å®ç°çš„ RAG é“¾ï¼Œé›†æˆçˆ¶å­ç´¢å¼•ç­–ç•¥ (Small-to-Big Retrieval)
    """

    def __init__(self, retriever, qa_prompt, history_prompt, llm, embeddings=None):
        self.retriever = retriever
        self.qa_prompt = qa_prompt
        self.history_prompt = history_prompt
        self.llm = llm
        self.output_parser = StrOutputParser()
        # è·å–çˆ¶æ–‡æ¡£å­˜å‚¨è·¯å¾„
        self.doc_store_path = getattr(config, "PARENT_DOC_STORE_PATH", "./doc_store")
        self.planner = QueryPlanner(llm)
        # CRAG æ–‡æ¡£è¯„åˆ†å™¨
        self.grader = DocumentGrader(llm)
        # æ˜¯å¦å¯ç”¨ CRAG
        self.use_crag = True
        # è¯­ä¹‰ç¼“å­˜ (éœ€è¦ä¼ å…¥ embeddings)
        self.semantic_cache = SemanticCache(embeddings) if embeddings else None
        self.use_semantic_cache = embeddings is not None
        # Token ç®¡ç†å™¨
        self.token_manager = TokenManager(max_context_tokens=6000)

    def _rewrite_question(self, question, chat_history):

        formatted_history_prompt = self.history_prompt.invoke({
            "chat_history": chat_history,
            "input": question
        })
        response = self.llm.invoke(formatted_history_prompt)
        return self.output_parser.invoke(response)

    def _map_children_to_parents(self, child_docs):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šå°†æ£€ç´¢åˆ°çš„å­å— (Child) æ˜ å°„å›çˆ¶å— (Parent)
        ä½¿ç”¨ SQLite æ‰¹é‡æŸ¥è¯¢æ›¿ä»£é€ä¸ª pickle æ–‡ä»¶è¯»å–
        """
        parent_docs = []
        seen_ids = set()
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ doc_ids
        doc_ids_to_fetch = []
        child_fallbacks = {}  # doc_id -> child_doc (ç”¨äºé™çº§)
        
        for child in child_docs:
            doc_id = child.metadata.get("doc_id")
            
            if not doc_id:
                # å…¼å®¹æ—§æ•°æ®ï¼šæ²¡æœ‰ ID çš„ç›´æ¥æ·»åŠ 
                if child.page_content not in [d.page_content for d in parent_docs]:
                    parent_docs.append(child)
                continue
                
            if doc_id not in seen_ids:
                doc_ids_to_fetch.append(doc_id)
                child_fallbacks[doc_id] = child
                seen_ids.add(doc_id)
        
        if not doc_ids_to_fetch:
            return parent_docs
        
        # æ‰¹é‡ä» SQLite è·å–çˆ¶æ–‡æ¡£
        db_path = getattr(config, "SQLITE_DB_PATH", "./doc_store.db")
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ä½¿ç”¨ IN å­å¥æ‰¹é‡æŸ¥è¯¢
            placeholders = ",".join("?" * len(doc_ids_to_fetch))
            cursor.execute(
                f"SELECT doc_id, data FROM parent_docs WHERE doc_id IN ({placeholders})",
                doc_ids_to_fetch
            )
            
            results = {row[0]: row[1] for row in cursor.fetchall()}
            conn.close()
            
            # æŒ‰åŸå§‹é¡ºåºå¤„ç†ç»“æœ
            for doc_id in doc_ids_to_fetch:
                if doc_id in results:
                    try:
                        parent_doc = pickle.loads(results[doc_id])
                        parent_docs.append(parent_doc)
                    except Exception as e:
                        print(f"ååºåˆ—åŒ–çˆ¶æ–‡æ¡£å¤±è´¥ {doc_id}: {e}")
                        parent_docs.append(child_fallbacks[doc_id])
                else:
                    # SQLite ä¸­æ‰¾ä¸åˆ°ï¼Œé™çº§ä½¿ç”¨å­å—
                    parent_docs.append(child_fallbacks[doc_id])
                    
        except Exception as e:
            print(f"SQLite æŸ¥è¯¢å¤±è´¥: {e}")
            # å…¨éƒ¨é™çº§ä½¿ç”¨å­å—
            for doc_id in doc_ids_to_fetch:
                parent_docs.append(child_fallbacks[doc_id])
        
        return parent_docs

    def _get_base_retriever(self):
        """
        è·å–åŸºç¡€æ£€ç´¢å™¨ï¼Œç”¨äºå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
        å¦‚æœæ˜¯ ContextualCompressionRetrieverï¼Œè¿”å›å…¶ base_retriever
        """
        if hasattr(self.retriever, 'base_retriever'):
            return self.retriever.base_retriever
        return self.retriever

    def _rerank_documents(self, docs, query):
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åºï¼ˆå¦‚æœå¯ç”¨äº† Rerankerï¼‰
        """
        if hasattr(self.retriever, 'base_compressor'):
            try:
                return list(self.retriever.base_compressor.compress_documents(docs, query))
            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥: {e}")
                return docs[:5]
        return docs[:5]

    def _deduplicate_docs(self, docs):
        """
        æ ¹æ® doc_id å¯¹æ–‡æ¡£å»é‡
        """
        seen_ids = set()
        unique_docs = []
        for doc in docs:
            doc_id = doc.metadata.get("doc_id")
            content_hash = hash(doc.page_content[:100]) if not doc_id else None
            key = doc_id or content_hash
            if key and key not in seen_ids:
                seen_ids.add(key)
                unique_docs.append(doc)
            elif not key:
                unique_docs.append(doc)
        return unique_docs

    def _apply_crag(self, question: str, docs: list, search_query: str) -> tuple:
        """
        åº”ç”¨ CRAG (Corrective RAG) çº é”™æ£€ç´¢
        
        Args:
            question: ç”¨æˆ·åŸå§‹é—®é¢˜
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            search_query: é‡å†™åçš„æœç´¢æŸ¥è¯¢
            
        Returns:
            (filtered_docs, crag_stats)
        """
        if not self.use_crag or not docs:
            return docs, {"crag_enabled": False}
        
        # è¯„ä¼°æ–‡æ¡£è´¨é‡
        filtered_docs, need_fallback, stats = self.grader.grade_and_filter(
            question, docs, threshold=0.5
        )
        
        crag_stats = {
            "crag_enabled": True,
            "original_count": stats["total"],
            "filtered_count": len(filtered_docs),
            "relevance_ratio": stats["relevance_ratio"],
            "fallback_triggered": need_fallback
        }
        
        # å¦‚æœéœ€è¦å›é€€ä¸”è¿‡æ»¤åæ–‡æ¡£å¤ªå°‘
        if need_fallback and len(filtered_docs) < 2:
            # å›é€€ç­–ç•¥ 1: ä½¿ç”¨ HyDE é‡è¯•
            print(f"âš ï¸ CRAG è§¦å‘å›é€€: ç›¸å…³æ€§æ¯”ä¾‹ {stats['relevance_ratio']:.1%}")
            
            hyde_doc = self.planner.generate_hyde_doc(question)
            if hyde_doc:
                enhanced_query = f"{search_query}\n{hyde_doc}"
                base_retriever = self._get_base_retriever()
                retry_docs = base_retriever.invoke(enhanced_query)
                
                # å†æ¬¡è¯„åˆ†
                retry_filtered, _, retry_stats = self.grader.grade_and_filter(
                    question, retry_docs, threshold=0.7
                )
                
                if retry_filtered:
                    filtered_docs.extend(retry_filtered)
                    filtered_docs = self._deduplicate_docs(filtered_docs)
                    crag_stats["hyde_retry"] = True
                    crag_stats["retry_added"] = len(retry_filtered)
        
        # å¦‚æœä»ç„¶æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ï¼Œè¿”å›åŸå§‹æ–‡æ¡£çš„å‰å‡ ä¸ª
        if len(filtered_docs) < 1:
            filtered_docs = docs[:3]
            crag_stats["fallback_to_original"] = True
        
        return filtered_docs, crag_stats

    def _prepare_context(self, input_dict: dict) -> dict:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡çš„è¾…åŠ©æ–¹æ³•ï¼ŒæŠ½å–æ£€ç´¢/è§„åˆ’é€»è¾‘ä¾› invoke å’Œ stream å…±ç”¨
        è¿”å›: {
            "run_id": str,
            "question": str,
            "chat_history": list,
            "search_query": str,
            "planning_type": str,
            "queries": list,
            "child_docs": list,
            "final_docs": list,
            "context_str": str,
            "formatted_qa_prompt": BaseMessage,
            "cache_hit": str | None
        }
        """
        run_id = str(uuid.uuid4())
        question = input_dict.get("input", "")
        chat_history = input_dict.get("chat_history", [])
        
        # æ£€æŸ¥ç¼“å­˜
        cache_hit = self._check_cache(question)
        if cache_hit:
            return {
                "run_id": run_id,
                "question": question,
                "chat_history": chat_history,
                "cache_hit": cache_hit
            }
        
        # å†å²è®°å½•å¤„ç† (æŒ‡ä»£æ¶ˆè§£)
        if chat_history:
            search_query = self._rewrite_question(question, chat_history)
        else:
            search_query = question
        
        # æ™ºèƒ½è·¯ç”± (ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»)
        plan_result = self.planner.plan(search_query, chat_history)
        planning_type = plan_result["type"]
        queries = plan_result["queries"]
        
        child_docs = []
        final_docs = []
        context_str = ""
        crag_stats = {"crag_enabled": False}
        
        if planning_type == "GREETING":
            # é—²èŠæ¨¡å¼ï¼šä¸æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
            pass
        
        elif planning_type == "COMPLEX":
            # å¤æ‚é—®é¢˜ï¼šå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            child_docs = self._deduplicate_docs(all_docs)
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # CRAG: è¯„ä¼°å¹¶è¿‡æ»¤æ–‡æ¡£
            child_docs, crag_stats = self._apply_crag(question, child_docs, search_query)
            
            final_docs = self._map_children_to_parents(child_docs)
            # Token ç®¡ç†ï¼šé™åˆ¶æ–‡æ¡£æ•°é‡å’Œä¸Šä¸‹æ–‡é•¿åº¦
            final_docs = self.token_manager.trim_documents(final_docs, max_docs=5)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
            context_str = self.token_manager.trim_context(context_str)
        
        elif planning_type == "ABSTRACT":
            # æŠ½è±¡é—®é¢˜ï¼šä½¿ç”¨ HyDE å¢å¼º
            hyde_doc = self.planner.generate_hyde_doc(search_query)
            final_query = f"{search_query}\n{hyde_doc}" if hyde_doc else search_query
            
            child_docs = self.retriever.invoke(final_query)
            
            # CRAG: è¯„ä¼°å¹¶è¿‡æ»¤æ–‡æ¡£
            child_docs, crag_stats = self._apply_crag(question, child_docs, search_query)
            
            final_docs = self._map_children_to_parents(child_docs)
            # Token ç®¡ç†ï¼šé™åˆ¶æ–‡æ¡£æ•°é‡å’Œä¸Šä¸‹æ–‡é•¿åº¦
            final_docs = self.token_manager.trim_documents(final_docs, max_docs=5)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
            context_str = self.token_manager.trim_context(context_str)
        
        else:  # SIMPLE
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            child_docs = self._deduplicate_docs(all_docs)
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # CRAG: è¯„ä¼°å¹¶è¿‡æ»¤æ–‡æ¡£
            child_docs, crag_stats = self._apply_crag(question, child_docs, search_query)
            
            final_docs = self._map_children_to_parents(child_docs)
            # Token ç®¡ç†ï¼šé™åˆ¶æ–‡æ¡£æ•°é‡å’Œä¸Šä¸‹æ–‡é•¿åº¦
            final_docs = self.token_manager.trim_documents(final_docs, max_docs=5)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
            context_str = self.token_manager.trim_context(context_str)
        
        # æ ¼å¼åŒ– QA æç¤ºè¯
        formatted_qa_prompt = self.qa_prompt.invoke({
            "chat_history": chat_history,
            "context": context_str,
            "question": question
        })
        
        return {
            "run_id": run_id,
            "question": question,
            "chat_history": chat_history,
            "search_query": search_query,
            "planning_type": planning_type,
            "queries": queries,
            "child_docs": child_docs,
            "final_docs": final_docs,
            "context_str": context_str,
            "formatted_qa_prompt": formatted_qa_prompt,
            "cache_hit": None,
            "crag_stats": crag_stats
        }

    def stream(self, input_dict: dict):
        """
        æµå¼ç”Ÿæˆå“åº”çš„æ–¹æ³•
        Yields:
            1. é¦–å…ˆ yield ä¸€ä¸ª dict åŒ…å«å…ƒæ•°æ® (source_documents, run_id ç­‰)
            2. ç„¶å yield æ–‡æœ¬ token (str)
        """
        start_time = time.time()
        question = input_dict.get("input", "")
        
        if not question:
            yield {"type": "metadata", "source_documents": [], "run_id": "", "error": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"}
            return
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        ctx = self._prepare_context(input_dict)
        
        # ç¼“å­˜å‘½ä¸­æ—¶ç›´æ¥è¿”å›
        if ctx.get("cache_hit"):
            yield {
                "type": "metadata",
                "source_documents": [],
                "run_id": ctx["run_id"],
                "cache_hit": True
            }
            yield ctx["cache_hit"]
            return
        
        # Yield å…ƒæ•°æ® (åŒ…å« source_documents ä¾›å‰ç«¯å±•ç¤ºæ¥æº)
        yield {
            "type": "metadata",
            "source_documents": ctx["final_docs"],
            "run_id": ctx["run_id"],
            "planning_type": ctx["planning_type"],
            "cache_hit": False
        }
        
        # æµå¼ç”Ÿæˆç­”æ¡ˆ
        full_answer = ""
        for chunk in self.llm.stream(ctx["formatted_qa_prompt"]):
            token = chunk.content if hasattr(chunk, 'content') else str(chunk)
            if token:
                full_answer += token
                yield token
        
        end_time = time.time()
        
        # ç”Ÿæˆå®Œæ¯•åï¼Œå¤„ç†æ—¥å¿—å’Œç¼“å­˜
        log_data = {
            "run_id": ctx["run_id"],
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "latency": end_time - start_time,
            "question": ctx["question"],
            "rewrite_query": ctx.get("search_query", ctx["question"]),
            "planning_type": ctx.get("planning_type", "UNKNOWN"),
            "expanded_queries": ctx.get("queries", [ctx["question"]]),
            "retrieved_doc_ids": [d.metadata.get("doc_id", "unknown") for d in ctx.get("child_docs", [])],
            "answer": full_answer
        }
        self._save_log(log_data)
        
        # å­˜å…¥ç¼“å­˜ (é—²èŠä¸ç¼“å­˜)
        if ctx.get("planning_type") != "GREETING" and full_answer:
            self._update_cache(ctx["question"], full_answer)

    def invoke(self, input_dict: dict):
        run_id = str(uuid.uuid4())
        start_time = time.time()
        question = input_dict.get("input", "")
        if not question:
            return {"answer": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚", "source_documents": []}
            
        chat_history = input_dict.get("chat_history", [])

        # 0. è¯­ä¹‰ç¼“å­˜ (Phase 4: Semantic Cache - ç®€å•å®ç°)
        cache_hit = self._check_cache(question)
        if cache_hit:
            return {
                "answer": cache_hit,
                "source_documents": [],
                "log_data": {"cache": "hit", "question": question}
            }

        # 1. å†å²è®°å½•å¤„ç† (æŒ‡ä»£æ¶ˆè§£)
        if chat_history:
            search_query = self._rewrite_question(question, chat_history)
        else:
            search_query = question

        # 2. æ™ºèƒ½è·¯ç”± (ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»)
        plan_result = self.planner.plan(search_query, chat_history)
        planning_type = plan_result["type"]
        queries = plan_result["queries"]
        use_hyde = plan_result["use_hyde"]
        
        child_docs = []
        final_docs = []
        context_str = ""
        
        if planning_type == "GREETING":
            # é—²èŠæ¨¡å¼ï¼šä¸æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
            pass
        
        elif planning_type == "COMPLEX":
            # å¤æ‚é—®é¢˜ï¼šå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            # å¹¶å‘æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢å˜ä½“
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            # å»é‡
            child_docs = self._deduplicate_docs(all_docs)
            
            # é‡æ’åºï¼ˆå¦‚æœæœ‰ Rerankerï¼‰
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # çˆ¶å­ç´¢å¼•ç½®æ¢
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        elif planning_type == "ABSTRACT":
            # æŠ½è±¡é—®é¢˜ï¼šä½¿ç”¨ HyDE å¢å¼º
            hyde_doc = self.planner.generate_hyde_doc(search_query)
            final_query = f"{search_query}\n{hyde_doc}" if hyde_doc else search_query
            
            child_docs = self.retriever.invoke(final_query)
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        else:  # SIMPLE
            # SIMPLE é—®é¢˜ç°åœ¨ä¹Ÿä½¿ç”¨å¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢ä»¥æé«˜å¬å›ç‡
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            # å¹¶å‘æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢å˜ä½“
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            # å»é‡
            child_docs = self._deduplicate_docs(all_docs)
            
            # é‡æ’åºï¼ˆå¦‚æœæœ‰ Rerankerï¼‰
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # çˆ¶å­ç´¢å¼•ç½®æ¢
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])

        # 6. ç”Ÿæˆç­”æ¡ˆ
        formatted_qa_prompt = self.qa_prompt.invoke({
            "chat_history": chat_history,
            "context": context_str,
            "question": question
        })

        ai_message = self.llm.invoke(formatted_qa_prompt)
        answer = self.output_parser.invoke(ai_message)

        end_time = time.time()
        
        # 7. æ•°æ®åŸ‹ç‚¹è®°å½• (Phase 3: Feedback Loop)
        log_data = {
            "run_id": run_id,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "latency": end_time - start_time,
            "question": question,
            "rewrite_query": search_query,
            "planning_type": planning_type,
            "expanded_queries": queries if planning_type in ["COMPLEX", "SIMPLE"] else [search_query],
            "retrieved_doc_ids": [d.metadata.get("doc_id", "unknown") for d in child_docs],
            "answer": answer
        }
        self._save_log(log_data)
        
        # å­˜å…¥ç¼“å­˜ (é—²èŠä¸ç¼“å­˜)
        if planning_type != "GREETING" and answer:
            self._update_cache(question, answer)

        return {
            "answer": answer,
            "source_documents": final_docs,
            "log_data": log_data,
            "run_id": run_id
        }

    def _check_cache(self, question):
        """æ£€æŸ¥ç¼“å­˜ - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰ç¼“å­˜"""
        # ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰ç¼“å­˜
        if self.use_semantic_cache and self.semantic_cache:
            result = self.semantic_cache.get(question)
            if result:
                return result
        
        # å›é€€åˆ°å­—ç¬¦ä¸²ç¼“å­˜
        cache_path = "./logs/cache.json"
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    cache = json.load(f)
                return cache.get(question)
            except:
                return None
        return None

    def _update_cache(self, question, answer):
        """æ›´æ–°ç¼“å­˜ - åŒæ—¶æ›´æ–°è¯­ä¹‰ç¼“å­˜å’Œå­—ç¬¦ä¸²ç¼“å­˜"""
        # æ›´æ–°è¯­ä¹‰ç¼“å­˜
        if self.use_semantic_cache and self.semantic_cache:
            self.semantic_cache.set(question, answer)
        
        # åŒæ—¶æ›´æ–°å­—ç¬¦ä¸²ç¼“å­˜ (ä½œä¸ºå¤‡ä»½)
        cache_path = "./logs/cache.json"
        os.makedirs("./logs", exist_ok=True)
        cache = {}
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                pass
        
        cache[question] = answer
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(cache) > 1000:
            first_key = next(iter(cache))
            del cache[first_key]
            
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

    def _save_log(self, log_data):
        """ä¿å­˜æ—¥å¿—ç”¨äºåç»­ A/B æµ‹è¯•å’Œè¯„ä¼°"""
        if "run_id" not in log_data:
            log_data["run_id"] = str(uuid.uuid4())
        run_id = log_data["run_id"]
        
        log_dir = "./logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, "rag_activity.jsonl")
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_data, ensure_ascii=False) + "\n")
        return run_id



@st.cache_resource
def load_embedding_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': device}
    )


def get_rag_chain(custom_prompt=None):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"è¿è¡Œè®¾å¤‡: {device}")

    # 1. å‘é‡æ£€ç´¢ (æ£€ç´¢å­å—)
    embeddings = load_embedding_model()
    if not os.path.exists(config.PERSIST_DIRECTORY):
        return None

    vectorstore = Chroma(
        persist_directory=config.PERSIST_DIRECTORY,
        embedding_function=embeddings
    )
    chroma_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 10, "fetch_k": 20, "lambda_mult": 0.7}
    )

    # 2. å…³é”®è¯æ£€ç´¢ (æ£€ç´¢å­å—)
    bm25_retriever = None
    if os.path.exists(config.BM25_PERSIST_PATH):
        try:
            with open(config.BM25_PERSIST_PATH, "rb") as f:
                bm25_docs = pickle.load(f)
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 10
            print(f"BM25 ç´¢å¼•å·²åŠ è½½ï¼Œæ–‡æ¡£æ•°: {len(bm25_docs)}")
        except Exception as e:
            print(f"BM25 åŠ è½½å¤±è´¥: {e}")

    # 3. æ··åˆæ£€ç´¢
    if bm25_retriever:
        ensemble_retriever = EnsembleRetriever(
            retrievers=[chroma_retriever, bm25_retriever],
            weights=[0.6, 0.4]
        )
    else:
        ensemble_retriever = chroma_retriever

    # 4. é‡æ’åº (å¯¹å­å—è¿›è¡Œæ’åº)
    # Rerank åº”è¯¥ä½œç”¨äºå­å—ï¼Œå› ä¸ºå­å—è¯­ä¹‰æ›´é›†ä¸­ï¼Œè¯„åˆ†æ›´å‡†
    final_retriever = ensemble_retriever
    if device == "cpu":
        print("CPUæ¨¡å¼ï¼šè·³è¿‡ Rerank æ­¥éª¤")
    else:
        try:
            print(f"åŠ è½½ Rerank æ¨¡å‹: {config.RERANKER_MODEL_NAME}")
            rerank_model = HuggingFaceCrossEncoder(
                model_name=config.RERANKER_MODEL_NAME,
                model_kwargs={'device': device}
            )
            compressor = CrossEncoderReranker(model=rerank_model, top_n=5)
            final_retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=ensemble_retriever
            )
        except Exception as e:
            print(f"Rerank åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§ä½¿ç”¨æ··åˆæ£€ç´¢: {e}")
            final_retriever = ensemble_retriever

    # 5. LLM & Prompt
    llm = ChatOpenAI(
        model=config.LLM_MODEL_NAME,
        openai_api_key=config.API_KEY,
        openai_api_base=config.BASE_URL,
        temperature=0.1
    )

    # å†å²è®°å½•é‡å†™æç¤ºè¯
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    history_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    # é—®ç­”æç¤ºè¯ (å«å¼•ç”¨æ ‡è®°æŒ‡ä»¤)
    default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢çš„ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
åœ¨å›ç­”ä¸­å¼•ç”¨ä¸Šä¸‹æ–‡æ—¶ï¼Œè¯·ä½¿ç”¨ [1], [2] è¿™æ ·çš„æ ¼å¼æ ‡æ³¨æ¥æºï¼Œå¯¹åº”ä¸Šä¸‹æ–‡ä¸­çš„ [æ–‡æ¡£ 1], [æ–‡æ¡£ 2] ç­‰ã€‚
å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä¸”èŠå¤©è®°å½•ä¹Ÿæ²¡æåˆ°ï¼Œè¯·æ‰¿è®¤ä¸çŸ¥é“ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘:
{context}
"""
    system_template = custom_prompt if custom_prompt else default_system_prompt
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{question}"),
    ])

    # è¿”å›æ”¯æŒçˆ¶å­ç´¢å¼•çš„ Chain (ä¼ å…¥ embeddings ä»¥å¯ç”¨è¯­ä¹‰ç¼“å­˜)
    return ManualHistoryRAGChain(final_retriever, qa_prompt, history_prompt, llm, embeddings=embeddings)