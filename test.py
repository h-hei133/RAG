import os
import shutil
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# è®¾å®šå‘é‡æ•°æ®åº“çš„å­˜å‚¨è·¯å¾„
PERSIST_DIRECTORY = "./chroma_db"


def ingest_document(file_path):
    """
    è¯»å–PDFï¼Œåˆ‡åˆ†ï¼Œå¹¶å­˜å‚¨åˆ°æœ¬åœ°å‘é‡åº“
    """
    # 1. åŠ è½½ PDF
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")

    loader = PyPDFLoader(file_path)
    documents = loader.load()
    print(f"å·²åŠ è½½æ–‡æ¡£ï¼Œå…± {len(documents)} é¡µ")

    # 2. åˆ‡åˆ†æ–‡æœ¬ (Chunking)
    # chunk_size=500: æ¯ä¸ªå—çº¦300-500å­—ï¼Œé€‚åˆè¯­ä¹‰å®Œæ•´æ€§
    # chunk_overlap=50: é‡å 50å­—ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    splits = text_splitter.split_documents(documents)
    print(f"æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

    # 3. åˆå§‹åŒ– Embedding æ¨¡å‹ (æœ¬åœ°è¿è¡Œï¼Œä½¿ç”¨ BAAI ä¸­æ–‡å°æ¨¡å‹)
    print("æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹ (é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5"
    )

    # 4. å­˜å…¥ ChromaDB (å¦‚æœå­˜åœ¨æ—§æ•°æ®ï¼Œå…ˆæ¸…ç†ï¼Œä¿è¯æ˜¯çº¯å‡€çš„çŸ¥è¯†åº“)
    if os.path.exists(PERSIST_DIRECTORY):
        shutil.rmtree(PERSIST_DIRECTORY)
        print("å·²æ¸…ç†æ—§çš„å‘é‡æ•°æ®åº“")

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=PERSIST_DIRECTORY
    )
    print("å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    return True


import os
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import config

# è®¾å®šå‘é‡æ•°æ®åº“è·¯å¾„
PERSIST_DIRECTORY = "./chroma_db"


def get_rag_chain():
    """
    åˆå§‹åŒ– RAG é“¾
    """
    # 1. å‡†å¤‡ Embedding (å¿…é¡»ä¸ ingestion.py ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹)
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5"
    )

    # 2. åŠ è½½å‘é‡æ•°æ®åº“
    if not os.path.exists(PERSIST_DIRECTORY):
        return None

    vectorstore = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embeddings
    )

    # 3. åˆ›å»ºæ£€ç´¢å™¨ (æ£€ç´¢æœ€ç›¸ä¼¼çš„3ä¸ªç‰‡æ®µ)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 4. åˆå§‹åŒ– LLM (æŒ‡å‘ DeepSeek)
    llm = ChatOpenAI(
        model=config.LLM_MODEL_NAME,
        openai_api_key=config.API_KEY,
        openai_api_base=config.BASE_URL,
        temperature=0.1  # RAG éœ€è¦ä¸¥è°¨ï¼Œæ¸©åº¦è°ƒä½
    )

    # 5. å®šä¹‰ Prompt æ¨¡æ¿
    template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢çš„ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”â€œæ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆâ€ï¼Œä¸è¦ç¼–é€ ã€‚

    ã€ä¸Šä¸‹æ–‡ã€‘:
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘:
    {question}

    å›ç­”:"""

    prompt = ChatPromptTemplate.from_template(template)

    # 6. æ„å»º LCEL é“¾ (LangChain Expression Language)
    rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    return rag_chain


def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])


import streamlit as st
import os
from src.ingestion import ingest_document
from src.rag_chain import get_rag_chain

st.set_page_config(page_title="ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹ (DeepSeekç‰ˆ)", layout="wide")

# æ ‡é¢˜
st.title("ğŸ¤– ä¸ªäººä¸“å±çŸ¥è¯†åº“åŠ©æ‰‹")
st.caption("Powered by DeepSeek-V3 + Local Embeddings")

# --- ä¾§è¾¹æ ï¼šæ–‡ä»¶ä¸Šä¼  ---
with st.sidebar:
    st.header("1. ä¸Šä¼ æ–‡æ¡£")
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼  PDF æ–‡æ¡£", type=["pdf"])

    if uploaded_file and not st.session_state.get("file_processed", False):
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“ï¼Œè¯·ç¨å€™..."):
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = os.path.join("data", "temp.pdf")
            os.makedirs("data", exist_ok=True)
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # è°ƒç”¨æ•°æ®å¤„ç†é€»è¾‘
            success = ingest_document(temp_path)
            if success:
                st.success("çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
                st.session_state["file_processed"] = True

    if st.button("é‡ç½®çŸ¥è¯†åº“"):
        st.session_state["file_processed"] = False
        st.rerun()

# --- ä¸»åŒºåŸŸï¼šèŠå¤© ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼è¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼Œç„¶åé—®æˆ‘å…³äºæ–‡æ¡£çš„é—®é¢˜ã€‚"}]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.session_state["messages"].append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # 2. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†æ–‡ä»¶
    if not st.session_state.get("file_processed"):
        response = "è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF æ–‡æ¡£ï¼Œæˆ‘æ‰èƒ½å›ç­”ä½ çš„é—®é¢˜å“¦ã€‚"
        st.session_state["messages"].append({"role": "assistant", "content": response})
        st.chat_message("assistant").write(response)
    else:
        # 3. è°ƒç”¨ RAG é“¾ç”Ÿæˆå›ç­”
        rag_chain = get_rag_chain()
        if rag_chain:
            with st.chat_message("assistant"):
                with st.spinner("DeepSeek æ­£åœ¨æ€è€ƒ..."):
                    try:
                        response = rag_chain.invoke(prompt)
                        st.write(response)
                        st.session_state["messages"].append({"role": "assistant", "content": response})
                    except Exception as e:
                        st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
        else:
            st.error("çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")