import os
import torch
import pickle
import sqlite3
import config
import streamlit as st
from operator import itemgetter

# LangChain ç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage

# æ£€ç´¢ç›¸å…³ç»„ä»¶
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
import json
import time
import uuid
from typing import Literal, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

def calculate_mrr(retrieved_ids: List[str], relevant_id: str) -> float:
    """
    è®¡ç®—å•ä¸ªæŸ¥è¯¢çš„å€’æ•°æ’å (Reciprocal Rank)
    è¿”å› 1/Kï¼Œå…¶ä¸­ K æ˜¯ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’åä½ç½®
    å¦‚æœç›¸å…³æ–‡æ¡£ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè¿”å› 0
    """
    try:
        rank = retrieved_ids.index(relevant_id) + 1
        return 1.0 / rank
    except ValueError:
        return 0.0


def log_feedback(run_id: str, score: int, comment: Optional[str] = None):
    """
    è®°å½•ç”¨æˆ·åé¦ˆåˆ°æ—¥å¿—æ–‡ä»¶
    score: 1 = æ­£é¢åé¦ˆ (ğŸ‘), 0 = è´Ÿé¢åé¦ˆ (ğŸ‘)
    """
    import config
    feedback_log = {
        "run_id": run_id,
        "score": score,
        "comment": comment,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    os.makedirs(os.path.dirname(config.FEEDBACK_LOG_PATH), exist_ok=True)
    with open(config.FEEDBACK_LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(feedback_log, ensure_ascii=False) + "\n")

from langchain_core.pydantic_v1 import BaseModel, Field

class RouteQuery(BaseModel):
    """ç”¨æˆ·æŸ¥è¯¢æ„å›¾åˆ†ç±»"""
    intent: Literal["GREETING", "SIMPLE", "COMPLEX", "ABSTRACT"] = Field(
        description="æŸ¥è¯¢æ„å›¾ç±»å‹"
    )
    reasoning: str = Field(
        description="åˆ†ç±»ç†ç”±",
        default=""
    )

class QueryPlanner:
    """
    Query è§„åˆ’å™¨ï¼šè´Ÿè´£ Query é‡å†™ã€åˆ†å‘ã€HyDE ç­‰é«˜çº§æ£€ç´¢ç­–ç•¥
    """
    def __init__(self, llm):
        self.llm = llm
        self.output_parser = StrOutputParser()

    def classify_intent(self, question: str) -> str:
        """
        ä½¿ç”¨ LLM å¯¹ç”¨æˆ·é—®é¢˜è¿›è¡Œæ„å›¾åˆ†ç±»
        è¿”å›: GREETING | SIMPLE | COMPLEX | ABSTRACT
        """
        routing_prompt = f"""åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­å…¶æ„å›¾ç±»å‹ã€‚

é—®é¢˜: "{question}"

ç±»å‹åˆ¤æ–­æ ‡å‡†:
1. GREETING: æ‰“æ‹›å‘¼æˆ–é—²èŠï¼ˆå¦‚ï¼šä½ å¥½ã€è°¢è°¢ã€å†è§ã€hiã€helloï¼‰
2. SIMPLE: äº‹å®æ€§ç®€å•é—®é¢˜ï¼Œåªéœ€å•ä¸€æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚ï¼šXXæ˜¯ä»€ä¹ˆæ—¶å€™å‘å¸ƒçš„ï¼Ÿï¼‰
3. COMPLEX: æ¶‰åŠå¯¹æ¯”ã€å¤šè·³æ¨ç†æˆ–éœ€è¦å¤šè§’åº¦å›ç­”ï¼ˆå¦‚ï¼šåä¸ºå’Œå°ç±³å“ªä¸ªå¥½ï¼Ÿï¼‰
4. ABSTRACT: æ¦‚å¿µæ€§é—®é¢˜ï¼Œé€‚åˆå…ˆç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆå†æ£€ç´¢ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿå¦‚ä½•ç†è§£XXï¼Ÿï¼‰

è¯·åªè¿”å›ä¸€ä¸ª JSON æ ¼å¼:
{{"intent": "ç±»å‹", "reasoning": "ç†ç”±"}}
"""
        try:
            response = self.llm.invoke(routing_prompt)
            result_text = self.output_parser.invoke(response)
            # å°è¯•è§£æ JSON
            import json
            import re
            # æå– JSON éƒ¨åˆ†
            json_match = re.search(r'\{[^}]+\}', result_text)
            if json_match:
                result = json.loads(json_match.group())
                intent = result.get("intent", "SIMPLE").upper()
                if intent in ["GREETING", "SIMPLE", "COMPLEX", "ABSTRACT"]:
                    return intent
            return "SIMPLE"
        except Exception as e:
            print(f"æ„å›¾åˆ†ç±»å¤±è´¥: {e}")
            return "SIMPLE"

    def plan(self, question: str, chat_history: list) -> dict:
        """
        å¯¹ Query è¿›è¡Œæ„å›¾åˆ†æå’Œè§„åˆ’
        """
        intent = self.classify_intent(question)
        
        result = {
            "type": intent,
            "queries": [question],
            "use_hyde": intent == "ABSTRACT"
        }
        
        # COMPLEX å’Œ SIMPLE é—®é¢˜éƒ½è¿›è¡ŒæŸ¥è¯¢æ‰©å±•ä»¥æé«˜å¬å›ç‡
        if intent in ["COMPLEX", "SIMPLE"]:
            variants = self.expand_query(question)
            result["queries"] = [question] + variants
        
        return result

    def expand_query(self, question: str) -> List[str]:
        """
        ç”ŸæˆæŸ¥è¯¢å˜ä½“ï¼Œç”¨äºå¤šè·¯å¬å›
        è¿”å› 3 ä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„æœç´¢è¯
        """
        prompt = f"""é’ˆå¯¹ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œè¯·ç”Ÿæˆ3ä¸ªä¸åŒè§’åº¦çš„æœç´¢æŸ¥è¯¢è¯ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¦æ±‚:
1. æ¯ä¸ªæŸ¥è¯¢è¯å ä¸€è¡Œ
2. æ¶µç›–é—®é¢˜çš„ä¸åŒæ–¹é¢
3. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ç»„åˆ
4. åªè¿”å›æŸ¥è¯¢è¯ï¼Œä¸è¦ç¼–å·æˆ–è§£é‡Š

ç¤ºä¾‹:
é—®é¢˜: "åä¸ºå’Œå°ç±³çš„æ‰‹æœºå“ªä¸ªå¥½ï¼Ÿ"
åä¸ºæ‰‹æœºå‚æ•°é…ç½®
å°ç±³æ‰‹æœºæ€§èƒ½è¯„æµ‹
åä¸ºå°ç±³å¯¹æ¯”åˆ†æ"""
        
        try:
            response = self.llm.invoke(prompt)
            result = self.output_parser.invoke(response)
            variants = [v.strip() for v in result.strip().split("\n") if v.strip()]
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„å˜ä½“
            variants = [v for v in variants if 2 < len(v) < 50]
            return variants[:3]  # æœ€å¤šè¿”å›3ä¸ª
        except Exception as e:
            print(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return []

    def generate_hyde_doc(self, question: str) -> str:
        """
        ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ (HyDE - Hypothetical Document Embeddings)
        ç”¨äºæŠ½è±¡æ¦‚å¿µé—®é¢˜çš„æ£€ç´¢å¢å¼º
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ä½œè€…ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œå†™ä¸€æ®µä¸“ä¸šã€å‡†ç¡®çš„å›ç­”è‰ç¨¿ã€‚
è¿™æ®µå›ç­”å°†ç”¨äºå¸®åŠ©æœç´¢å¼•æ“æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ‰€ä»¥è¯·åŒ…å«å°½å¯èƒ½å¤šçš„ä¸“ä¸šæœ¯è¯­å’Œå…³é”®æ¦‚å¿µã€‚

é—®é¢˜: {question}

è¦æ±‚:
1. 100-200å­—å·¦å³
2. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
3. æ¶µç›–æ ¸å¿ƒæ¦‚å¿µ
4. ä¸è¦è¯´"æˆ‘ä¸çŸ¥é“"ä¹‹ç±»çš„è¯"""
        
        try:
            response = self.llm.invoke(prompt)
            return self.output_parser.invoke(response)
        except Exception as e:
            print(f"HyDEç”Ÿæˆå¤±è´¥: {e}")
            return ""


class ManualHistoryRAGChain:
    """
    æ‰‹åŠ¨å®ç°çš„ RAG é“¾ï¼Œé›†æˆçˆ¶å­ç´¢å¼•ç­–ç•¥ (Small-to-Big Retrieval)
    """

    def __init__(self, retriever, qa_prompt, history_prompt, llm):
        self.retriever = retriever
        self.qa_prompt = qa_prompt
        self.history_prompt = history_prompt
        self.llm = llm
        self.output_parser = StrOutputParser()
        # è·å–çˆ¶æ–‡æ¡£å­˜å‚¨è·¯å¾„
        self.doc_store_path = getattr(config, "PARENT_DOC_STORE_PATH", "./doc_store")
        self.planner = QueryPlanner(llm)

    def _rewrite_question(self, question, chat_history):

        formatted_history_prompt = self.history_prompt.invoke({
            "chat_history": chat_history,
            "input": question
        })
        response = self.llm.invoke(formatted_history_prompt)
        return self.output_parser.invoke(response)

    def _map_children_to_parents(self, child_docs):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šå°†æ£€ç´¢åˆ°çš„å­å— (Child) æ˜ å°„å›çˆ¶å— (Parent)
        ä½¿ç”¨ SQLite æ‰¹é‡æŸ¥è¯¢æ›¿ä»£é€ä¸ª pickle æ–‡ä»¶è¯»å–
        """
        parent_docs = []
        seen_ids = set()
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ doc_ids
        doc_ids_to_fetch = []
        child_fallbacks = {}  # doc_id -> child_doc (ç”¨äºé™çº§)
        
        for child in child_docs:
            doc_id = child.metadata.get("doc_id")
            
            if not doc_id:
                # å…¼å®¹æ—§æ•°æ®ï¼šæ²¡æœ‰ ID çš„ç›´æ¥æ·»åŠ 
                if child.page_content not in [d.page_content for d in parent_docs]:
                    parent_docs.append(child)
                continue
                
            if doc_id not in seen_ids:
                doc_ids_to_fetch.append(doc_id)
                child_fallbacks[doc_id] = child
                seen_ids.add(doc_id)
        
        if not doc_ids_to_fetch:
            return parent_docs
        
        # æ‰¹é‡ä» SQLite è·å–çˆ¶æ–‡æ¡£
        db_path = getattr(config, "SQLITE_DB_PATH", "./doc_store.db")
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ä½¿ç”¨ IN å­å¥æ‰¹é‡æŸ¥è¯¢
            placeholders = ",".join("?" * len(doc_ids_to_fetch))
            cursor.execute(
                f"SELECT doc_id, data FROM parent_docs WHERE doc_id IN ({placeholders})",
                doc_ids_to_fetch
            )
            
            results = {row[0]: row[1] for row in cursor.fetchall()}
            conn.close()
            
            # æŒ‰åŸå§‹é¡ºåºå¤„ç†ç»“æœ
            for doc_id in doc_ids_to_fetch:
                if doc_id in results:
                    try:
                        parent_doc = pickle.loads(results[doc_id])
                        parent_docs.append(parent_doc)
                    except Exception as e:
                        print(f"ååºåˆ—åŒ–çˆ¶æ–‡æ¡£å¤±è´¥ {doc_id}: {e}")
                        parent_docs.append(child_fallbacks[doc_id])
                else:
                    # SQLite ä¸­æ‰¾ä¸åˆ°ï¼Œé™çº§ä½¿ç”¨å­å—
                    parent_docs.append(child_fallbacks[doc_id])
                    
        except Exception as e:
            print(f"SQLite æŸ¥è¯¢å¤±è´¥: {e}")
            # å…¨éƒ¨é™çº§ä½¿ç”¨å­å—
            for doc_id in doc_ids_to_fetch:
                parent_docs.append(child_fallbacks[doc_id])
        
        return parent_docs

    def _get_base_retriever(self):
        """
        è·å–åŸºç¡€æ£€ç´¢å™¨ï¼Œç”¨äºå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
        å¦‚æœæ˜¯ ContextualCompressionRetrieverï¼Œè¿”å›å…¶ base_retriever
        """
        if hasattr(self.retriever, 'base_retriever'):
            return self.retriever.base_retriever
        return self.retriever

    def _rerank_documents(self, docs, query):
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åºï¼ˆå¦‚æœå¯ç”¨äº† Rerankerï¼‰
        """
        if hasattr(self.retriever, 'base_compressor'):
            try:
                return list(self.retriever.base_compressor.compress_documents(docs, query))
            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥: {e}")
                return docs[:5]
        return docs[:5]

    def _deduplicate_docs(self, docs):
        """
        æ ¹æ® doc_id å¯¹æ–‡æ¡£å»é‡
        """
        seen_ids = set()
        unique_docs = []
        for doc in docs:
            doc_id = doc.metadata.get("doc_id")
            content_hash = hash(doc.page_content[:100]) if not doc_id else None
            key = doc_id or content_hash
            if key and key not in seen_ids:
                seen_ids.add(key)
                unique_docs.append(doc)
            elif not key:
                unique_docs.append(doc)
        return unique_docs

    def _prepare_context(self, input_dict: dict) -> dict:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡çš„è¾…åŠ©æ–¹æ³•ï¼ŒæŠ½å–æ£€ç´¢/è§„åˆ’é€»è¾‘ä¾› invoke å’Œ stream å…±ç”¨
        è¿”å›: {
            "run_id": str,
            "question": str,
            "chat_history": list,
            "search_query": str,
            "planning_type": str,
            "queries": list,
            "child_docs": list,
            "final_docs": list,
            "context_str": str,
            "formatted_qa_prompt": BaseMessage,
            "cache_hit": str | None
        }
        """
        run_id = str(uuid.uuid4())
        question = input_dict.get("input", "")
        chat_history = input_dict.get("chat_history", [])
        
        # æ£€æŸ¥ç¼“å­˜
        cache_hit = self._check_cache(question)
        if cache_hit:
            return {
                "run_id": run_id,
                "question": question,
                "chat_history": chat_history,
                "cache_hit": cache_hit
            }
        
        # å†å²è®°å½•å¤„ç† (æŒ‡ä»£æ¶ˆè§£)
        if chat_history:
            search_query = self._rewrite_question(question, chat_history)
        else:
            search_query = question
        
        # æ™ºèƒ½è·¯ç”± (ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»)
        plan_result = self.planner.plan(search_query, chat_history)
        planning_type = plan_result["type"]
        queries = plan_result["queries"]
        
        child_docs = []
        final_docs = []
        context_str = ""
        
        if planning_type == "GREETING":
            # é—²èŠæ¨¡å¼ï¼šä¸æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
            pass
        
        elif planning_type == "COMPLEX":
            # å¤æ‚é—®é¢˜ï¼šå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            child_docs = self._deduplicate_docs(all_docs)
            child_docs = self._rerank_documents(child_docs, search_query)
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        elif planning_type == "ABSTRACT":
            # æŠ½è±¡é—®é¢˜ï¼šä½¿ç”¨ HyDE å¢å¼º
            hyde_doc = self.planner.generate_hyde_doc(search_query)
            final_query = f"{search_query}\n{hyde_doc}" if hyde_doc else search_query
            
            child_docs = self.retriever.invoke(final_query)
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        else:  # SIMPLE
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            child_docs = self._deduplicate_docs(all_docs)
            child_docs = self._rerank_documents(child_docs, search_query)
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        # æ ¼å¼åŒ– QA æç¤ºè¯
        formatted_qa_prompt = self.qa_prompt.invoke({
            "chat_history": chat_history,
            "context": context_str,
            "question": question
        })
        
        return {
            "run_id": run_id,
            "question": question,
            "chat_history": chat_history,
            "search_query": search_query,
            "planning_type": planning_type,
            "queries": queries,
            "child_docs": child_docs,
            "final_docs": final_docs,
            "context_str": context_str,
            "formatted_qa_prompt": formatted_qa_prompt,
            "cache_hit": None
        }

    def stream(self, input_dict: dict):
        """
        æµå¼ç”Ÿæˆå“åº”çš„æ–¹æ³•
        Yields:
            1. é¦–å…ˆ yield ä¸€ä¸ª dict åŒ…å«å…ƒæ•°æ® (source_documents, run_id ç­‰)
            2. ç„¶å yield æ–‡æœ¬ token (str)
        """
        start_time = time.time()
        question = input_dict.get("input", "")
        
        if not question:
            yield {"type": "metadata", "source_documents": [], "run_id": "", "error": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚"}
            return
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        ctx = self._prepare_context(input_dict)
        
        # ç¼“å­˜å‘½ä¸­æ—¶ç›´æ¥è¿”å›
        if ctx.get("cache_hit"):
            yield {
                "type": "metadata",
                "source_documents": [],
                "run_id": ctx["run_id"],
                "cache_hit": True
            }
            yield ctx["cache_hit"]
            return
        
        # Yield å…ƒæ•°æ® (åŒ…å« source_documents ä¾›å‰ç«¯å±•ç¤ºæ¥æº)
        yield {
            "type": "metadata",
            "source_documents": ctx["final_docs"],
            "run_id": ctx["run_id"],
            "planning_type": ctx["planning_type"],
            "cache_hit": False
        }
        
        # æµå¼ç”Ÿæˆç­”æ¡ˆ
        full_answer = ""
        for chunk in self.llm.stream(ctx["formatted_qa_prompt"]):
            token = chunk.content if hasattr(chunk, 'content') else str(chunk)
            if token:
                full_answer += token
                yield token
        
        end_time = time.time()
        
        # ç”Ÿæˆå®Œæ¯•åï¼Œå¤„ç†æ—¥å¿—å’Œç¼“å­˜
        log_data = {
            "run_id": ctx["run_id"],
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "latency": end_time - start_time,
            "question": ctx["question"],
            "rewrite_query": ctx.get("search_query", ctx["question"]),
            "planning_type": ctx.get("planning_type", "UNKNOWN"),
            "expanded_queries": ctx.get("queries", [ctx["question"]]),
            "retrieved_doc_ids": [d.metadata.get("doc_id", "unknown") for d in ctx.get("child_docs", [])],
            "answer": full_answer
        }
        self._save_log(log_data)
        
        # å­˜å…¥ç¼“å­˜ (é—²èŠä¸ç¼“å­˜)
        if ctx.get("planning_type") != "GREETING" and full_answer:
            self._update_cache(ctx["question"], full_answer)

    def invoke(self, input_dict: dict):
        run_id = str(uuid.uuid4())
        start_time = time.time()
        question = input_dict.get("input", "")
        if not question:
            return {"answer": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚", "source_documents": []}
            
        chat_history = input_dict.get("chat_history", [])

        # 0. è¯­ä¹‰ç¼“å­˜ (Phase 4: Semantic Cache - ç®€å•å®ç°)
        cache_hit = self._check_cache(question)
        if cache_hit:
            return {
                "answer": cache_hit,
                "source_documents": [],
                "log_data": {"cache": "hit", "question": question}
            }

        # 1. å†å²è®°å½•å¤„ç† (æŒ‡ä»£æ¶ˆè§£)
        if chat_history:
            search_query = self._rewrite_question(question, chat_history)
        else:
            search_query = question

        # 2. æ™ºèƒ½è·¯ç”± (ä½¿ç”¨ LLM è¿›è¡Œæ„å›¾åˆ†ç±»)
        plan_result = self.planner.plan(search_query, chat_history)
        planning_type = plan_result["type"]
        queries = plan_result["queries"]
        use_hyde = plan_result["use_hyde"]
        
        child_docs = []
        final_docs = []
        context_str = ""
        
        if planning_type == "GREETING":
            # é—²èŠæ¨¡å¼ï¼šä¸æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆ
            pass
        
        elif planning_type == "COMPLEX":
            # å¤æ‚é—®é¢˜ï¼šå¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            # å¹¶å‘æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢å˜ä½“
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            # å»é‡
            child_docs = self._deduplicate_docs(all_docs)
            
            # é‡æ’åºï¼ˆå¦‚æœæœ‰ Rerankerï¼‰
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # çˆ¶å­ç´¢å¼•ç½®æ¢
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        elif planning_type == "ABSTRACT":
            # æŠ½è±¡é—®é¢˜ï¼šä½¿ç”¨ HyDE å¢å¼º
            hyde_doc = self.planner.generate_hyde_doc(search_query)
            final_query = f"{search_query}\n{hyde_doc}" if hyde_doc else search_query
            
            child_docs = self.retriever.invoke(final_query)
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])
        
        else:  # SIMPLE
            # SIMPLE é—®é¢˜ç°åœ¨ä¹Ÿä½¿ç”¨å¹¶å‘å¤šæŸ¥è¯¢æ£€ç´¢ä»¥æé«˜å¬å›ç‡
            base_retriever = self._get_base_retriever()
            all_docs = []
            
            # å¹¶å‘æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢å˜ä½“
            with ThreadPoolExecutor(max_workers=min(len(queries), 3)) as executor:
                future_to_query = {
                    executor.submit(base_retriever.invoke, q): q 
                    for q in queries
                }
                for future in as_completed(future_to_query):
                    try:
                        docs = future.result()
                        all_docs.extend(docs)
                    except Exception as e:
                        print(f"æ£€ç´¢å¤±è´¥: {e}")
            
            # å»é‡
            child_docs = self._deduplicate_docs(all_docs)
            
            # é‡æ’åºï¼ˆå¦‚æœæœ‰ Rerankerï¼‰
            child_docs = self._rerank_documents(child_docs, search_query)
            
            # çˆ¶å­ç´¢å¼•ç½®æ¢
            final_docs = self._map_children_to_parents(child_docs)
            context_str = "\n\n".join([f"[æ–‡æ¡£ {i+1}]: {d.page_content}" for i, d in enumerate(final_docs)])

        # 6. ç”Ÿæˆç­”æ¡ˆ
        formatted_qa_prompt = self.qa_prompt.invoke({
            "chat_history": chat_history,
            "context": context_str,
            "question": question
        })

        ai_message = self.llm.invoke(formatted_qa_prompt)
        answer = self.output_parser.invoke(ai_message)

        end_time = time.time()
        
        # 7. æ•°æ®åŸ‹ç‚¹è®°å½• (Phase 3: Feedback Loop)
        log_data = {
            "run_id": run_id,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "latency": end_time - start_time,
            "question": question,
            "rewrite_query": search_query,
            "planning_type": planning_type,
            "expanded_queries": queries if planning_type in ["COMPLEX", "SIMPLE"] else [search_query],
            "retrieved_doc_ids": [d.metadata.get("doc_id", "unknown") for d in child_docs],
            "answer": answer
        }
        self._save_log(log_data)
        
        # å­˜å…¥ç¼“å­˜ (é—²èŠä¸ç¼“å­˜)
        if planning_type != "GREETING" and answer:
            self._update_cache(question, answer)

        return {
            "answer": answer,
            "source_documents": final_docs,
            "log_data": log_data,
            "run_id": run_id
        }

    def _check_cache(self, question):
        """ç®€å•æœ¬åœ°ç¼“å­˜æ£€æŸ¥"""
        cache_path = "./logs/cache.json"
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    cache = json.load(f)
                return cache.get(question)
            except:
                return None
        return None

    def _update_cache(self, question, answer):
        """æ›´æ–°æœ¬åœ°ç¼“å­˜"""
        cache_path = "./logs/cache.json"
        os.makedirs("./logs", exist_ok=True)
        cache = {}
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                pass
        
        cache[question] = answer
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(cache) > 1000:
            first_key = next(iter(cache))
            del cache[first_key]
            
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

    def _save_log(self, log_data):
        """ä¿å­˜æ—¥å¿—ç”¨äºåç»­ A/B æµ‹è¯•å’Œè¯„ä¼°"""
        if "run_id" not in log_data:
            log_data["run_id"] = str(uuid.uuid4())
        run_id = log_data["run_id"]
        
        log_dir = "./logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, "rag_activity.jsonl")
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_data, ensure_ascii=False) + "\n")
        return run_id



@st.cache_resource
def load_embedding_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': device}
    )


def get_rag_chain(custom_prompt=None):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"è¿è¡Œè®¾å¤‡: {device}")

    # 1. å‘é‡æ£€ç´¢ (æ£€ç´¢å­å—)
    embeddings = load_embedding_model()
    if not os.path.exists(config.PERSIST_DIRECTORY):
        return None

    vectorstore = Chroma(
        persist_directory=config.PERSIST_DIRECTORY,
        embedding_function=embeddings
    )
    chroma_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 10, "fetch_k": 20, "lambda_mult": 0.7}
    )

    # 2. å…³é”®è¯æ£€ç´¢ (æ£€ç´¢å­å—)
    bm25_retriever = None
    if os.path.exists(config.BM25_PERSIST_PATH):
        try:
            with open(config.BM25_PERSIST_PATH, "rb") as f:
                bm25_docs = pickle.load(f)
            bm25_retriever = BM25Retriever.from_documents(bm25_docs)
            bm25_retriever.k = 10
            print(f"BM25 ç´¢å¼•å·²åŠ è½½ï¼Œæ–‡æ¡£æ•°: {len(bm25_docs)}")
        except Exception as e:
            print(f"BM25 åŠ è½½å¤±è´¥: {e}")

    # 3. æ··åˆæ£€ç´¢
    if bm25_retriever:
        ensemble_retriever = EnsembleRetriever(
            retrievers=[chroma_retriever, bm25_retriever],
            weights=[0.6, 0.4]
        )
    else:
        ensemble_retriever = chroma_retriever

    # 4. é‡æ’åº (å¯¹å­å—è¿›è¡Œæ’åº)
    # Rerank åº”è¯¥ä½œç”¨äºå­å—ï¼Œå› ä¸ºå­å—è¯­ä¹‰æ›´é›†ä¸­ï¼Œè¯„åˆ†æ›´å‡†
    final_retriever = ensemble_retriever
    if device == "cpu":
        print("CPUæ¨¡å¼ï¼šè·³è¿‡ Rerank æ­¥éª¤")
    else:
        try:
            print(f"åŠ è½½ Rerank æ¨¡å‹: {config.RERANKER_MODEL_NAME}")
            rerank_model = HuggingFaceCrossEncoder(
                model_name=config.RERANKER_MODEL_NAME,
                model_kwargs={'device': device}
            )
            compressor = CrossEncoderReranker(model=rerank_model, top_n=5)
            final_retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=ensemble_retriever
            )
        except Exception as e:
            print(f"Rerank åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§ä½¿ç”¨æ··åˆæ£€ç´¢: {e}")
            final_retriever = ensemble_retriever

    # 5. LLM & Prompt
    llm = ChatOpenAI(
        model=config.LLM_MODEL_NAME,
        openai_api_key=config.API_KEY,
        openai_api_base=config.BASE_URL,
        temperature=0.1
    )

    # å†å²è®°å½•é‡å†™æç¤ºè¯
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    history_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    # é—®ç­”æç¤ºè¯ (å«å¼•ç”¨æ ‡è®°æŒ‡ä»¤)
    default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢çš„ã€ä¸Šä¸‹æ–‡ã€‘å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
åœ¨å›ç­”ä¸­å¼•ç”¨ä¸Šä¸‹æ–‡æ—¶ï¼Œè¯·ä½¿ç”¨ [1], [2] è¿™æ ·çš„æ ¼å¼æ ‡æ³¨æ¥æºï¼Œå¯¹åº”ä¸Šä¸‹æ–‡ä¸­çš„ [æ–‡æ¡£ 1], [æ–‡æ¡£ 2] ç­‰ã€‚
å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä¸”èŠå¤©è®°å½•ä¹Ÿæ²¡æåˆ°ï¼Œè¯·æ‰¿è®¤ä¸çŸ¥é“ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘:
{context}
"""
    system_template = custom_prompt if custom_prompt else default_system_prompt
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{question}"),
    ])

    # è¿”å›æ”¯æŒçˆ¶å­ç´¢å¼•çš„ Chain
    return ManualHistoryRAGChain(final_retriever, qa_prompt, history_prompt, llm)